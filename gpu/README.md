# GPU

XXX: to be written

## Currently available GPUs for LLM/VLM workloads

- NVIDIA A100 - huge availability but already getting outdated
- NVIDIA H100 - 2-3x faster than A100 (half precision), 6x faster for fp8
- AMD MI250 ~= A100 - very few clouds have them
- AMD MI300 ~= H100 - donâ€™t expect until 1.5-2 years from now to be GA
- Google TPUs - lock-in
- Intel Gaudi2 ~= H100 - very difficult to find
- GraphCore - very difficult to find
